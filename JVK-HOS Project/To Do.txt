To Do:


Created scaled rowsX
Put code into jupyter notebookX
Look up "how to read a corrplot"
	-also compare the decison tree feature selection, after decison tree is implemented
Implement decsion Tree
     -compare features with corrplot
Implement GridSearch CV with select KbestX
	-in both knn and decision treeX
Implement randomforest with gridsearch x
	- gridsearch on all features x
	- if short on time do trees first- cuse this is slow af
	-take x_important based on data x?
	-remember scaling is not neccesary for decision trees! x
	-trim the tree iteratively? maybe
Get actual codes from healthcodeslenX
Organize feature lists into groupsX
Create best features list for each modelX
Grid search linear regressionX
get null values with kfolds methodX - do standard null too maybe
add month featureX
add parsed healthcode featureX
put average values and linear values in ensmbler
implement stochastic regressionX
	-make sure to get regression specific parameters if they are different from sdgc classifierX

use reguralization and other feature reduction methods found in:https://www.youtube.com/watch?v=91si52nk3LA&list=PL5-da3qGB5IB-Xdpj_uXJpLGiRfv9UVXI
Get the coeffecients of the best features


Implement ensembling
	- you can even use decision trees
	- as how to weight with regression models
Maybe stuff for later:
	make scaling pipeline faster
	look up pruning trees
	look at the algorthms used in the ensemble notebook
	cluster the procedures

Questions:
	Should I tune other parameters in random forest?(leaf size.. etc.)
	Effect of random_state on score, how should this be accounted for Answered below
	Do I need to do the feature reduction in the way done in the notebook, or could I use a cross validation?
	Should I remove duplicated dummies like spinal cord stimulator2? -Maybe, maybe not, should probably be cool. 
	What is the logic behind max features = 1? A: Basically just randomly looks at one feature
	Why does the order of features matter? A:Basically beacuse of random hashing? I believe, same thing with randomstate
	Increase the # of CV folds? A:maybe till aroud 5
	How is pca explained via my dataset - part A: By potenitally reducing noise -Maybe try
	How to choose the correct threshold for feature selection
	How to deal with all the healthcodes A
	How to implement grid search succesfully with the transformers Answered
	Is my method of cross validating the null results accurate?A;Kfold
	How to weight the regression ensemble's
	How to get a legend with the scatter plot
	What are the exact reasons for the super large RMSE values -Linear regression is very senistive 
        to irreleavnt features, shouldn't be anything to worry about
	
	Why are Kfolds different each time, but CV's the same -Aske george on slack
	Is my explanation for the lines(the steeper, less vairiable line make better features) correct?(Also can a line go > 45*?
	Should I be worried about overfitting as I have Iteratively gone thorugh my features? Specifically with my project? Should I go with
	the stepwise or iterative?
        
11.0595009685
17.1580231676
